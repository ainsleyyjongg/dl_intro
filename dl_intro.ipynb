{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ainsleyyjongg/dl_intro/blob/main/dl_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXG2Gc34Vqio"
      },
      "source": [
        "# What you will learn in this notebook\n",
        "\n",
        "1. Introduction to supervised learning and classification\n",
        "1. Introduction to loss functions, activation functions, and gradient descent\n",
        "1. Introduction to Pytorch packages, tensors, computation graphs, and gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1vtIxur97UM"
      },
      "source": [
        "## Introduction to supervised learning\n",
        "\n",
        "Machine learning tasks can be loosely grouped into three categories\n",
        "1. Supervised Learning\n",
        "1. Unsupervised Learning\n",
        "1. Reinforcement Learning\n",
        "\n",
        "In this workshop we will practice **supervised learning** and mostly **classification** problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDrlL0gLhGsW"
      },
      "source": [
        "Usually we are given a set of $n$ data pairs:\n",
        "\n",
        "$$dataset=\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}$$\n",
        "\n",
        "The assumption is that there is some unknown function $p$ which \"maps\" each $x_i$ to the corresponding $y_i$. The meaning of the word \"map\" is different for regression and classification.\n",
        "\n",
        "The regression case is straightforward, $y_i=p(x_i)$. In the classification case, $p$ is regarded as a conditional probability. Given a set of $C$ classes $\\{0,1,2,3,\\ldots,C-1 \\}$, $p(x_i)$ is interpreted as the **probability** that $x_i$ belongs to one of the $C$ classes.\n",
        "\n",
        "Our goal is to determine (learn), or at least approximate, $p$ from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGWS3hVuhGsW"
      },
      "source": [
        "In this notebook we give the first example of a classification problem. We are given a set (CIFAR10) of image,label pairs where each image can be in one of the __ten__ classes: ship, horse, car...etc and so each label is a value between 0 and 9 denoting the class of image. For example an image with an associated label of 8 is that of a ship.\n",
        "\n",
        "Coming back to the $x,y$ pairs mentioned earlier, the \"x\" is the image and the \"y\" is the class.\n",
        "\n",
        "To simplify matters we will group all \"inanimates\" (ship, car,...) into one class and all animate things (horse, dog,...) into another. Then $p$ becomes the probability of an input image being a \"machine\" or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpJEiPpbHjjk"
      },
      "source": [
        "## CIFAR10 Dataset\n",
        "\n",
        "At this point we will be needing two packages: torch and torchvision so we import them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j6mq7QsZzsF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision as vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5s-82k8IYwW"
      },
      "source": [
        "There are two datasets associated with CIFAR10: a training set and a testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXoGGW1OaAEZ"
      },
      "outputs": [],
      "source": [
        "cifar10_train=vision.datasets.CIFAR10(\".\",download=True,train=True)# train=True is the default\n",
        "cifar10_test=vision.datasets.CIFAR10(\".\",download=True,train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiK2f9uyH2uH"
      },
      "source": [
        "It helps to get an idea of the properties of the dataset and how to use torch datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tHVh-P7IQ8h"
      },
      "outputs": [],
      "source": [
        "train_samples=len(cifar10_train)\n",
        "test_samples=len(cifar10_test)\n",
        "print(\"The length of training data is {} and the test data is {}\".format(train_samples,test_samples))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JziiKBlKnDqH"
      },
      "outputs": [],
      "source": [
        "# Retrieve the first (image,label) pair\n",
        "img0,label0=cifar10_train[0]\n",
        "print(type(img0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuyZNWfIJmml"
      },
      "outputs": [],
      "source": [
        "# Plot the first image (a frog) and set the corresponding label as title\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig=plt.figure()\n",
        "fig.set_size_inches(1,1)\n",
        "p=fig.add_subplot()\n",
        "\n",
        "p.set_title(str(label0))\n",
        "p.axes.get_xaxis().set_visible(False)\n",
        "p.axes.get_yaxis().set_visible(False)\n",
        "plt.imshow(img0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI3ML83Va19K"
      },
      "source": [
        "In this notebook we will use a less \"conventional\" way of accessing the data items.\n",
        "- cifar10_train.data is an array of images so cifar10_train.data[0] is the first image\n",
        "- cifar10_train.targets is an array of the corresponding labels so cifar10_train.targets[0] is the first label.\n",
        "\n",
        "Next we create torch **tensors** from the datasets. For now, we think of a torch **tensor** as a multidimensional array.\n",
        "\n",
        "\n",
        "**Note**: the pixel values are divided by the maximal value (255). This is ofen the case to aid with the convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rOOuE5ra-qC"
      },
      "outputs": [],
      "source": [
        "img_train=torch.tensor(cifar10_train.data,dtype=torch.float32)/255.\n",
        "img_test=torch.tensor(cifar10_test.data,dtype=torch.float32)/255.\n",
        "label_train=torch.tensor(cifar10_train.targets,dtype=torch.float32)\n",
        "label_test=torch.tensor(cifar10_test.targets,dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4udcJFDWf8-8"
      },
      "source": [
        "\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "In this notebook we introduce Logistic Regression which can be regarded as the **simplest neural network**, a single \"neuron\". This type of network is sometime called a Perceptron, but the method used for learning is different from the way a Perceptron learns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFacVlcrhGsY"
      },
      "source": [
        "As can be seen from the figure in the next slide the input is a vector of size _n_ and it feeds a single unit (a neuron or perceptron). To obtain the output we perform the **dot** product between the matrix **W** and the input **x** and the result is fed into some function (usually nonlinear) _f_\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "z&=\\sum_iw_i\\cdot x_i+b\\\\\n",
        "\\hat{y}(x)&=f(z)\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf1hlIFXhGsY"
      },
      "source": [
        "Since $z$ depends on $w$ and $b$ so does $\\hat{y}$. The input and _f_ are known whereas _W_ and _b_ are parameters to be determined. Our goal is to find the _optimal_ _W_ and _b_ such that the output is as *close as possible* to the label associated with the input.\n",
        "![title](https://github.com/hikmatfarhat-ndu/CSC645/blob/master/figures/perceptron.png?raw=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMs5Y-KphGsZ"
      },
      "source": [
        "How is **as close as possible** defined? The dataset is usually a set of pairs $(x,y)$. We define the loss as the **deviation** between the label $y$ and the result $\\hat{y}=f(z)$\n",
        "\n",
        "$$loss=E_{w,b}(y,\\hat{y})$$\n",
        "\n",
        "The function $E$ depends on the problem (for example binary cross entropy, mean squared error,...)\n",
        "\n",
        "Note that $E$ depends on the parameters $w,b$. Our goal is to find the **optimal** $w,b$ such that the loss is minimal. From calculus we know that to find the minimum (max) of a function we compute its derivative and find where it is null."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wha5x_KeRHkw"
      },
      "source": [
        "## Sigmoid\n",
        "\n",
        "So far we have not specified the function _f_ that our  model depends on $\\hat{y}=f(z)$. In this example we use the **sigmoid** function. Given an input _z_ it has the form\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\sigma=\\frac{1}{1+e^{-z}}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "We can plot the sigmoid function using matplotlib. As you can see below the values of $\\sigma$ go from 0 to 1 which we interpret as a probability. For example, if $\\sigma=0.65$ then the probability of the image being a **ship** is 0.65 and of **not** being one is 0.35 so decide it is a ship.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAe6yJrBRN_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "z=[1/(1+np.exp(-x)) for x in range(-10,11)]\n",
        "plt.plot([x for x in range(-10,11)],z)\n",
        "plt.xticks([t for t in range(-10,11,2)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTPp2UMLhGsZ"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Now that we have an expression to optimize we need a method to find the optimal parameters. Typically, one computes the __gradient__ and the optimal value corresponds to the value  of the parameters when the __gradient vanishes__. Unfortunately, for logistic regression there is __no closed form solution__ so we seek a numerical method to find the optimal parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08C4C1wUcyQv"
      },
      "source": [
        "\n",
        "Our goal is to find the **optimal** values for _W_ and _b_. To do so we give them some _arbitrary_ values and then using the expression for $E$\n",
        "In the figure below we show an arbitrary function _E(w)_. For a given value of _w_ we compute the derivative (slope) of _E_ with respect to _w_ (two different values are shown). The point on the left side has a negative slope so we need to **increase** the value of _w_ to move toward the minimum whereas the point on the right side the slope is positive so we have to **decrease** the value of _w_."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_W8-x8O3imgQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYst_unphGsZ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/hikmatfarhat-ndu/CSC645/blob/master/figures/gradient-descent.png?raw=1\" width=\"350\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTBn8oi0hGsZ"
      },
      "source": [
        "In general we \"update\" the values of _w_ and _b_ as follows\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  w=w-\\alpha\\cdot \\frac{\\partial E}{\\partial w}\\\\\n",
        "  b=b-\\alpha\\cdot \\frac{\\partial E}{\\partial b}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a parameter chosen by us, called the __learning rate__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vMc-jV9pIcd"
      },
      "source": [
        "## Flatening the images\n",
        "The images have dimensions (3,32,32) (3 channels, 32 height,32 width). To feed them to our \"neuron\" we need to create a vector of dimension 3x32x32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UvukYwIicPqx"
      },
      "outputs": [],
      "source": [
        "dim=3*32*32\n",
        "train_samples=50000\n",
        "test_samples=10000\n",
        "img_train=img_train.reshape(train_samples,dim)\n",
        "img_test=img_test.reshape(test_samples,dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7QpEeoqppWW"
      },
      "source": [
        "As mentioned before, to simplify the problem, we group all inanimate entities together by giving them the label 1 and all animate things together by giving them the label 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WuNp40w4vebE"
      },
      "outputs": [],
      "source": [
        "#airplane=0,car=1,bird=2,cat=3,deer=4,dog=5,frog=6,horse=7,ship=8,truck=9\n",
        "features=torch.tensor([0,1,8,9])\n",
        "for i in range(label_train.shape[0]):\n",
        "    if torch.isin(label_train[i],features):\n",
        "        label_train[i]=1\n",
        "    else:\n",
        "        label_train[i]=0\n",
        "\n",
        "for i in range(label_test.shape[0]):\n",
        "    if torch.isin(label_test[i],features):\n",
        "        label_test[i]=1\n",
        "    else:\n",
        "        label_test[i]=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1V2d5mOt5FK",
        "outputId": "c65baefe-a457-4788-d1e6-bf7c54d77718"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(20000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# the dataset is a bit biased\n",
        "torch.count_nonzero(label_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-VxiVGnp2LC"
      },
      "source": [
        "## Initialize the parameters\n",
        "Our goal is to find the **optimal** values for the parameters, weights and bias. Intially we give them random values (for weights) and 0 for the bias as shown below. Note two things\n",
        "1. We divide the initial values of the weights by the total number of samples to minimize the possibility of divergence.\n",
        "1. The `reguires_grad` declares a tensor to be a variable, i.e. we need the derivative. In previous versions of Pytorch one needed to declare variables explicitly but this is deprecated now. See [here](https://pytorch.org/docs/stable/autograd.html#variable-deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "koYSysX4gKjr"
      },
      "outputs": [],
      "source": [
        "weights=torch.rand(dim,requires_grad=True,dtype=torch.float32)\n",
        "weights.data/=train_samples\n",
        "bias=torch.tensor(0.,requires_grad=True,dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z9ssRJxX6iw"
      },
      "source": [
        "## Optimization loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwNpWU0RoMbi",
        "outputId": "d27b588d-d29b-496a-f076-7c914cf0599f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.6942\n",
            "loss 0.4717\n",
            "loss 0.4585\n",
            "loss 0.4523\n",
            "loss 0.4480\n",
            "loss 0.4448\n",
            "loss 0.4423\n",
            "loss 0.4401\n",
            "loss 0.4384\n",
            "loss 0.4368\n"
          ]
        }
      ],
      "source": [
        "# learning rate\n",
        "rate=0.015\n",
        "# for 0-1 classification we use the binary cross entropy loss\n",
        "loss_fn=torch.nn.BCELoss()\n",
        "\n",
        "for i in range(1000):\n",
        "  y_hat=torch.matmul(img_train,weights)+bias\n",
        "  y_hat=torch.sigmoid(y_hat)\n",
        "  loss=loss_fn(y_hat.squeeze(),label_train)\n",
        " # compute the gradient wrt weights and bias\n",
        "  dw,db=torch.autograd.grad(loss,[weights,bias])\n",
        "  #update the weights and bias\n",
        "  weights.data-=rate*dw\n",
        "  bias.data-=rate*db\n",
        "\n",
        "  if(i%100==0):\n",
        "    print(\"loss {:.4f}\".format(loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJGEPlrz5l0U"
      },
      "source": [
        "## Prediction on the test data\n",
        "\n",
        "An important measure of any ML method is how well it \"generalizes\". This is done by using the trained model on test data, i.e. other than the data it was trained on. To do that we note that the output of our model is the probability that the input is an inanimate object, which could be any value between 0 and 1. The test labels are discrete values of 0 and 1 so how do we compare them? We regard a probability $\\ge 0.5$ to be 1 and $< 0.5$ to be 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XttSuF7xuVom"
      },
      "outputs": [],
      "source": [
        "def predict(X):\n",
        "    # m is the number of samples\n",
        "    m = X.size()[0]\n",
        "    # initialize the prediction variable\n",
        "    label_prediction = torch.zeros(m,1)\n",
        "\n",
        "    # Compute vector \"y_hat\" predicting\n",
        "    # the probabilities of a machine being present in the picture\n",
        "\n",
        "    y_hat=torch.matmul(X,weights)+bias\n",
        "    y_hat=torch.sigmoid(y_hat)\n",
        "    # loop over all samples\n",
        "    for i in range(y_hat.size()[0]):\n",
        "        # Convert probabilities y_hat[0,i] to actual predictions\n",
        "        #if y_hat[i]>=0.5:\n",
        "        if y_hat[i]>=0.5:\n",
        "            label_prediction[i]=1\n",
        "        else:\n",
        "            label_prediction[i]=0\n",
        "\n",
        "\n",
        "    return label_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LDDffgKuv5P",
        "outputId": "e9f3e507-cea0-4afd-9273-c938fa32cda0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy:tensor(81.3940)\n",
            "test accuracy:tensor(81.1900)\n"
          ]
        }
      ],
      "source": [
        "label_prediction_test = predict(img_test).squeeze()\n",
        "label_prediction_train = predict(img_train).squeeze()\n",
        "print(\"train accuracy:\"+str((100 - torch.mean(torch.abs(label_prediction_train - label_train)) * 100)))\n",
        "print(\"test accuracy:\"+str((100 - torch.mean(torch.abs(label_prediction_test - label_test)) * 100)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdCBzLnqc6tO"
      },
      "source": [
        "## Using Pytorch\n",
        "\n",
        "The model used in this notebooks is simple enough to code directly. We only needed Pytorch to compute the gradients. When more complicated models are used this process becomes unwieldy. We can use Pytorch to abstract away the details.  \n",
        "The abstractions offered by Pytorch are illustrated below to solve the same problem that we just did. In later notebooks,  the training procedure is more or less the same, only the model and the optimizer will differ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1baF5ll16Q3"
      },
      "source": [
        "### The model\n",
        "\n",
        "The model we plan to use is encapsulated in a class that **inherits** from ```torch.nn.Module```\n",
        "\n",
        "All we need to do is **override** two methods:\n",
        "1. ```__init__```. As you would have guess this is called when the object is constructed to initialize our model\n",
        "1. ``` forward```. This is called everytime a forward computation is needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5LlED-xrgbiJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,in_features,out_features):\n",
        "    super(Net, self).__init__()\n",
        "    self.input_size=in_features\n",
        "    self.output_size=out_features\n",
        "    # declaring weights and bias as parameters so that they are included\n",
        "    # in the return value of .parameters()\n",
        "    self.weights=nn.Parameter(torch.rand(in_features,requires_grad=True,dtype=torch.float32))\n",
        "    self.weights.data/=in_features\n",
        "    self.bias=nn.Parameter(torch.tensor(0.,requires_grad=True,dtype=torch.float32))\n",
        "    #self.layer=nn.Linear(self.input_size,self.output_size,bias=True)\n",
        "  def forward(self,input):\n",
        "    y_hat=torch.matmul(input,self.weights)+self.bias\n",
        "    y_hat=torch.sigmoid(y_hat)\n",
        "    return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm160c2I23J3"
      },
      "source": [
        "Note that in the initialization, the weights and bias are constructed as ```Parameter```. This is so that we can use the ```.parameters()``` call and pass it to the optimizer.\n",
        "Next we create an instance of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Y9_n6Y2N00Io"
      },
      "outputs": [],
      "source": [
        "model=Net(dim,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKkSOrAn35u5"
      },
      "source": [
        "Recall that each learning iteration performs a number of steps.\n",
        "- Compute the forward pass over the input to get the output. This is now done using ```model.forward()``` indirectly by calling ```model(input)```\n",
        "- Compute the loss using an appropriate loss function. Same as before\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG8AIGO9hGsc"
      },
      "source": [
        "- Compute the gradients. Now we use ```loss.backward()```. Not only it computes the gradient with respect to the parameters but saves those values in the parameters themselves. For example, if ```p``` is a parameters then ```loss.backward()``` computes the gradient and saves it in ```p.grad```\n",
        "- Update the parameters. This is done by the optimizer using ```optimizer.step()```. This is important since later on we will use optimizers that use a different strategy to update the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L9FtlGbz1S3",
        "outputId": "6adba644-396d-4753-f3b0-5aaf3d1393e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0.7170\n",
            "loss 0.4717\n",
            "loss 0.4585\n",
            "loss 0.4523\n",
            "loss 0.4480\n",
            "loss 0.4448\n",
            "loss 0.4423\n",
            "loss 0.4401\n",
            "loss 0.4384\n",
            "loss 0.4368\n"
          ]
        }
      ],
      "source": [
        "rate=0.015\n",
        "\n",
        "import torch.optim as optim\n",
        "optimizer=optim.SGD(model.parameters(),lr=rate)\n",
        "loss_fn=torch.nn.BCELoss()\n",
        "\n",
        "for i in range(1000):\n",
        "  # uses the .forward() method to get y_hat\n",
        "  y_hat=model(img_train)\n",
        "  # as before\n",
        "  loss=loss_fn(y_hat.squeeze(),label_train)\n",
        "  # Computes the gradients and saves them in the appropriate .grad\n",
        "  loss.backward()\n",
        "  # updates the parameters using the computed .grad\n",
        "  optimizer.step()\n",
        "  # zero the .grad values so that they don't accumulate\n",
        "  optimizer.zero_grad()\n",
        "  if(i%100==0):\n",
        "    print(\"loss {:.4f}\".format(loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0WBei4N6o35"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}